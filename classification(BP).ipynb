{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "819eac660725ab0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.autograd import Variable\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import integrate\n",
    "from model import LSTM\n",
    "from random import sample\n",
    "import torch.nn as nn\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:42:10.028829200Z",
     "start_time": "2023-12-21T09:41:54.862751300Z"
    }
   },
   "id": "85bd25f01d5d0469"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare CPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d24b8002293f8cf2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f315d7dc0b0430a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Dataset\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16c004982bf28bd9"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of train set: 220\n",
      "lenght of valid set: 40\n",
      "valid set: ['C4_27', 'C1_24', 'C2_24', 'C6_3', 'C6_7', 'C2_13', 'C2_14', 'C5_33', 'C1_41', 'C3_32', 'C2_48', 'C3_42', 'C1_27', 'C3_25', 'C2_21', 'C4_13', 'C5_5', 'C2_7', 'C3_41', 'C5_30', 'C3_23', 'C5_50', 'C5_43', 'C6_20', 'C3_49', 'C5_12', 'C3_2', 'C1_29', 'C3_43', 'C2_18', 'C4_28', 'C2_27', 'C6_37', 'C5_25', 'C2_1', 'C5_9', 'C6_14', 'C5_28', 'C6_11', 'C1_37']\n",
      "lenght of test set: 40\n",
      "test set: ['C3_35', 'C3_17', 'C2_2', 'C4_9', 'C4_31', 'C1_3', 'C5_23', 'C2_30', 'C5_20', 'C4_18', 'C4_34', 'C4_46', 'C5_3', 'C5_16', 'C6_24', 'C6_22', 'C4_39', 'C3_46', 'C2_19', 'C3_36', 'C4_32', 'C5_10', 'C3_8', 'C2_40', 'C1_20', 'C5_8', 'C4_24', 'C1_19', 'C4_14', 'C5_15', 'C2_31', 'C1_49', 'C3_24', 'C1_43', 'C1_40', 'C5_40', 'C6_23', 'C6_47', 'C6_8', 'C1_44']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Index = ['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6']\n",
    "\n",
    "Data_path = 'D:/Workspace/TENG-Signal-Classification/dataset/preprocessed'\n",
    "\n",
    "cases = os.listdir(Data_path)\n",
    "\n",
    "random.shuffle(cases)\n",
    "## Load data\n",
    "test_set = cases[:40]\n",
    "\n",
    "valid_set = cases[40:80]\n",
    "\n",
    "train_set = cases[80:]\n",
    "\n",
    "print('lenght of train set:', len(train_set))\n",
    "print('lenght of valid set:', len(valid_set))\n",
    "print('valid set:', valid_set)\n",
    "print('lenght of test set:', len(test_set))\n",
    "print('test set:', test_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:42:10.050869900Z",
     "start_time": "2023-12-21T09:42:10.031822100Z"
    }
   },
   "id": "824119aea61bd461"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd96ab4ab9b1896"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| loading train set data..... |\n",
      "| done |\n",
      "| loading valid set data..... |\n",
      "| done |\n",
      "| loading test set data..... |\n",
      "| done |\n",
      "| train set data to tensor..... |\n",
      "| done |\n",
      "| valid data to tensor..... |\n",
      "| done |\n",
      "| test data to tensor..... |\n",
      "| done |\n"
     ]
    }
   ],
   "source": [
    "x_train_data = [] \n",
    "y_train_data = []\n",
    "x_valid_data = []\n",
    "y_valid_data = []\n",
    "x_test_data = []\n",
    "y_test_data = []\n",
    "\n",
    "print('| loading train set data..... |')\n",
    "for case in train_set:\n",
    "    DATA = {}\n",
    "    label = case.split('_')[0]\n",
    "    #   Read csv\n",
    "    file = case + '.csv'\n",
    "    data = pd.read_csv(Data_path + '/' + case + '/' + file, usecols = Index)\n",
    "    #   Convert lable into int\n",
    "    Encode_label = {\n",
    "            'C1': 0,\n",
    "            'C2': 1,\n",
    "            'C3': 2,\n",
    "            'C4': 3,\n",
    "            'C5': 4,\n",
    "            'C6': 5, \n",
    "    }\n",
    "    entropy = pd.value_counts(data['Channel1']) / len(data['Channel1'])\n",
    "    DATA['C1'] = [sum(data['Channel1'].to_numpy()),np.max((data['Channel1'].to_numpy())),np.min((data['Channel1'].to_numpy())),np.mean((data['Channel1'].to_numpy())),np.std((data['Channel1'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel2']) / len(data['Channel2'])\n",
    "    DATA['C2'] = [sum(data['Channel2'].to_numpy()),np.max((data['Channel2'].to_numpy())),np.min((data['Channel2'].to_numpy())),np.mean((data['Channel2'].to_numpy())),np.std((data['Channel2'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel3']) / len(data['Channel3'])\n",
    "    DATA['C3'] = [sum(data['Channel3'].to_numpy()),np.max((data['Channel3'].to_numpy())),np.min((data['Channel3'].to_numpy())),np.mean((data['Channel3'].to_numpy())),np.std((data['Channel3'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel4']) / len(data['Channel4'])\n",
    "    DATA['C4'] = [sum(data['Channel4'].to_numpy()),np.max((data['Channel4'].to_numpy())),np.min((data['Channel4'].to_numpy())),np.mean((data['Channel4'].to_numpy())),np.std((data['Channel4'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel5']) / len(data['Channel5'])\n",
    "    DATA['C5'] = [sum(data['Channel5'].to_numpy()),np.max((data['Channel5'].to_numpy())),np.min((data['Channel5'].to_numpy())),np.mean((data['Channel5'].to_numpy())),np.std((data['Channel5'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel6']) / len(data['Channel6'])\n",
    "    DATA['C6'] = [sum(data['Channel6'].to_numpy()),np.max((data['Channel6'].to_numpy())),np.min((data['Channel6'].to_numpy())),np.mean((data['Channel6'].to_numpy())),np.std((data['Channel6'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    # x_train_data.append(data)\n",
    "    x_train_data.append(pd.DataFrame.from_dict(DATA, orient='index').T)\n",
    "    y_train_data.append(Encode_label[label])\n",
    "print('| done |')\n",
    "\n",
    "print('| loading valid set data..... |')\n",
    "for case in valid_set:\n",
    "    DATA = {}\n",
    "    label = case.split('_')[0]\n",
    "    #   Read csv\n",
    "    file = case + '.csv'\n",
    "    data = pd.read_csv(Data_path + '/' + case + '/' + file, usecols = Index)\n",
    "    #   Convert lable into int\n",
    "    Encode_label = {\n",
    "            'C1': 0,\n",
    "            'C2': 1,\n",
    "            'C3': 2,\n",
    "            'C4': 3,\n",
    "            'C5': 4,\n",
    "            'C6': 5, \n",
    "    }\n",
    "    entropy = pd.value_counts(data['Channel1']) / len(data['Channel1'])\n",
    "    DATA['C1'] = [sum(data['Channel1'].to_numpy()),np.max((data['Channel1'].to_numpy())),np.min((data['Channel1'].to_numpy())),np.mean((data['Channel1'].to_numpy())),np.std((data['Channel1'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel2']) / len(data['Channel2'])\n",
    "    DATA['C2'] = [sum(data['Channel2'].to_numpy()),np.max((data['Channel2'].to_numpy())),np.min((data['Channel2'].to_numpy())),np.mean((data['Channel2'].to_numpy())),np.std((data['Channel2'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel3']) / len(data['Channel3'])\n",
    "    DATA['C3'] = [sum(data['Channel3'].to_numpy()),np.max((data['Channel3'].to_numpy())),np.min((data['Channel3'].to_numpy())),np.mean((data['Channel3'].to_numpy())),np.std((data['Channel3'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel4']) / len(data['Channel4'])\n",
    "    DATA['C4'] = [sum(data['Channel4'].to_numpy()),np.max((data['Channel4'].to_numpy())),np.min((data['Channel4'].to_numpy())),np.mean((data['Channel4'].to_numpy())),np.std((data['Channel4'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel5']) / len(data['Channel5'])\n",
    "    DATA['C5'] = [sum(data['Channel5'].to_numpy()),np.max((data['Channel5'].to_numpy())),np.min((data['Channel5'].to_numpy())),np.mean((data['Channel5'].to_numpy())),np.std((data['Channel5'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel6']) / len(data['Channel6'])\n",
    "    DATA['C6'] = [sum(data['Channel6'].to_numpy()),np.max((data['Channel6'].to_numpy())),np.min((data['Channel6'].to_numpy())),np.mean((data['Channel6'].to_numpy())),np.std((data['Channel6'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    # x_valid_data.append(data)\n",
    "    x_valid_data.append(pd.DataFrame.from_dict(DATA, orient='index').T)\n",
    "    y_valid_data.append(Encode_label[label])\n",
    "print('| done |')\n",
    "\n",
    "print('| loading test set data..... |')\n",
    "for case in test_set:\n",
    "    DATA = {}\n",
    "    label = case.split('_')[0]\n",
    "    #   Read csv\n",
    "    file = case + '.csv'\n",
    "    data = pd.read_csv(Data_path + '/' + case + '/' + file, usecols = Index)\n",
    "    #   Convert lable into int\n",
    "    Encode_label = {\n",
    "            'C1': 0,\n",
    "            'C2': 1,\n",
    "            'C3': 2,\n",
    "            'C4': 3,\n",
    "            'C5': 4,\n",
    "            'C6': 5, \n",
    "    }\n",
    "    entropy = pd.value_counts(data['Channel1']) / len(data['Channel1'])\n",
    "    DATA['C1'] = [sum(data['Channel1'].to_numpy()),np.max((data['Channel1'].to_numpy())),np.min((data['Channel1'].to_numpy())),np.mean((data['Channel1'].to_numpy())),np.std((data['Channel1'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel2']) / len(data['Channel2'])\n",
    "    DATA['C2'] = [sum(data['Channel2'].to_numpy()),np.max((data['Channel2'].to_numpy())),np.min((data['Channel2'].to_numpy())),np.mean((data['Channel2'].to_numpy())),np.std((data['Channel2'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel3']) / len(data['Channel3'])\n",
    "    DATA['C3'] = [sum(data['Channel3'].to_numpy()),np.max((data['Channel3'].to_numpy())),np.min((data['Channel3'].to_numpy())),np.mean((data['Channel3'].to_numpy())),np.std((data['Channel3'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel4']) / len(data['Channel4'])\n",
    "    DATA['C4'] = [sum(data['Channel4'].to_numpy()),np.max((data['Channel4'].to_numpy())),np.min((data['Channel4'].to_numpy())),np.mean((data['Channel4'].to_numpy())),np.std((data['Channel4'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel5']) / len(data['Channel5'])\n",
    "    DATA['C5'] = [sum(data['Channel5'].to_numpy()),np.max((data['Channel5'].to_numpy())),np.min((data['Channel5'].to_numpy())),np.mean((data['Channel5'].to_numpy())),np.std((data['Channel5'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    entropy = pd.value_counts(data['Channel6']) / len(data['Channel6'])\n",
    "    DATA['C6'] = [sum(data['Channel6'].to_numpy()),np.max((data['Channel6'].to_numpy())),np.min((data['Channel6'].to_numpy())),np.mean((data['Channel6'].to_numpy())),np.std((data['Channel6'].to_numpy())),sum(np.log2(entropy) * entropy * (-1))]\n",
    "    # x_test_data.append(data)\n",
    "    x_test_data.append(pd.DataFrame.from_dict(DATA, orient='index').T)\n",
    "    y_test_data.append(Encode_label[label])\n",
    "print('| done |')\n",
    "\n",
    "ss = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_valid = []\n",
    "Y_valid = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "\n",
    "print('| train set data to tensor..... |')\n",
    "## To tensors\n",
    "for i in range(len(train_set)):\n",
    "    X = x_train_data[i]\n",
    "    Y = y_train_data[i]\n",
    "    X_ss = ss.fit_transform(X)\n",
    "\n",
    "    X_train.append(np.asarray(X_ss).flatten())\n",
    "    Y_train.append(Y)\n",
    "print('| done |')\n",
    "\n",
    "print('| valid data to tensor..... |')\n",
    "for i in range(len(valid_set)):\n",
    "    X = x_valid_data[i]\n",
    "    Y = y_valid_data[i]\n",
    "    X_ss = ss.fit_transform(X)\n",
    "    \n",
    "    X_valid.append(np.asarray(X_ss).flatten())\n",
    "    Y_valid.append(Y)\n",
    "print('| done |')\n",
    "\n",
    "print('| test data to tensor..... |')\n",
    "for i in range(len(test_set)):\n",
    "    X = x_test_data[i]\n",
    "    Y = y_test_data[i]\n",
    "    X_ss = ss.fit_transform(X)\n",
    "\n",
    "    X_test.append(np.asarray(X_ss).flatten())\n",
    "    Y_test.append(Y)\n",
    "print('| done |')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:46:05.131442300Z",
     "start_time": "2023-12-21T09:46:01.645223700Z"
    }
   },
   "id": "c87fbbfab918529c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training(BP)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eacad5fdda097c71"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "334a91e9f6ab626e"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "MLPClassifier(hidden_layer_sizes=(64, 16), max_iter=3000, momentum=0.85,\n              random_state=0, solver='sgd')",
      "text/html": "<style>#sk-container-id-18 {color: black;}#sk-container-id-18 pre{padding: 0;}#sk-container-id-18 div.sk-toggleable {background-color: white;}#sk-container-id-18 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-18 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-18 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-18 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-18 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-18 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-18 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-18 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-18 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-18 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-18 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-18 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-18 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-18 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-18 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-18 div.sk-item {position: relative;z-index: 1;}#sk-container-id-18 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-18 div.sk-item::before, #sk-container-id-18 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-18 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-18 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-18 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-18 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-18 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-18 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-18 div.sk-label-container {text-align: center;}#sk-container-id-18 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-18 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-18\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(64, 16), max_iter=3000, momentum=0.85,\n              random_state=0, solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" checked><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(64, 16), max_iter=3000, momentum=0.85,\n              random_state=0, solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128,), activation='relu', solver='sgd',  nesterovs_momentum=True, momentum=0.85, alpha=0.0001, max_iter=3000, random_state = 128)\n",
    "mlp.fit(X_train, np.asarray(Y_train)) \n"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:46:59.864584100Z",
     "start_time": "2023-12-21T09:46:56.214176600Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.725\n"
     ]
    }
   ],
   "source": [
    "Y_pred = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test,Y_pred)\n",
    "print(accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:46:59.873606300Z",
     "start_time": "2023-12-21T09:46:59.867576700Z"
    }
   },
   "id": "3804c5480f033a23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
