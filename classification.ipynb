{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2023-12-18T09:59:32.701375100Z",
     "start_time": "2023-12-18T09:59:32.692736200Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.classification.accuracy import Accuracy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.autograd import Variable \n",
    "from tqdm.auto import tqdm\n",
    "from scipy import integrate\n",
    "from model import LSTM\n",
    "from random import sample\n",
    "import torch.nn as nn\n",
    "from scipy.signal import find_peaks\n",
    "from livelossplot import PlotLosses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare CPU"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(torch.__version__ )\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Index = ['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6']\n",
    "\n",
    "Data_path = 'D:/Workspace/TENG-Signal-Classification/dataset/preprocessed'\n",
    "\n",
    "cases = os.listdir(Data_path)\n",
    "\n",
    "random.shuffle(cases)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set = cases[:50]\n",
    "\n",
    "valid_set = cases[50:100]\n",
    "\n",
    "train_set = cases[100:]\n",
    "\n",
    "print('lenght of train set:', len(train_set))\n",
    "print('lenght of valid set:', len(valid_set))\n",
    "print('valid set:', valid_set)\n",
    "print('lenght of test set:', len(test_set))\n",
    "print('test set:', test_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train_data = [] \n",
    "y_train_data = []\n",
    "x_valid_data = []\n",
    "y_valid_data = []\n",
    "x_test_data = []\n",
    "y_test_data = []\n",
    "\n",
    "print('| loading train set data..... |')\n",
    "for case in train_set:\n",
    "    label = case.split('_')[0]\n",
    "    #   Read csv\n",
    "    file = case + '.csv'\n",
    "    data = pd.read_csv(Data_path + '/' + case + '/' + file, usecols = Index)\n",
    "    #   Convert lable into int\n",
    "    Encode_label = {\n",
    "            'C1': 0,\n",
    "            'C2': 1,\n",
    "            'C3': 2,\n",
    "            'C4': 3,\n",
    "            'C5': 4,\n",
    "            'C6': 5, \n",
    "    }\n",
    "    x_train_data.append(data)\n",
    "    y_train_data.append(Encode_label[label])\n",
    "print('| done |')\n",
    "\n",
    "print('| loading valid set data..... |')\n",
    "for case in valid_set:\n",
    "    label = case.split('_')[0]\n",
    "    #   Read csv\n",
    "    file = case + '.csv'\n",
    "    data = pd.read_csv(Data_path + '/' + case + '/' + file, usecols = Index)\n",
    "    #   Convert lable into int\n",
    "    Encode_label = {\n",
    "            'C1': 0,\n",
    "            'C2': 1,\n",
    "            'C3': 2,\n",
    "            'C4': 3,\n",
    "            'C5': 4,\n",
    "            'C6': 5, \n",
    "    }\n",
    "    x_valid_data.append(data)\n",
    "    y_valid_data.append(Encode_label[label])\n",
    "print('| done |')\n",
    "\n",
    "print('| loading test set data..... |')\n",
    "for case in test_set:\n",
    "    label = case.split('_')[0]\n",
    "    #   Read csv\n",
    "    file = case + '.csv'\n",
    "    data = pd.read_csv(Data_path + '/' + case + '/' + file, usecols = Index)\n",
    "    #   Convert lable into int\n",
    "    Encode_label = {\n",
    "            'C1': 0,\n",
    "            'C2': 1,\n",
    "            'C3': 2,\n",
    "            'C4': 3,\n",
    "            'C5': 4,\n",
    "            'C6': 5, \n",
    "    }\n",
    "    x_test_data.append(data)\n",
    "    y_test_data.append(Encode_label[label])\n",
    "print('| done |')\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train_tensors = []\n",
    "Y_train_tensors = []\n",
    "X_valid_tensors = []\n",
    "Y_valid_tensors = []\n",
    "X_test_tensors = []\n",
    "Y_test_tensors = []\n",
    "\n",
    "\n",
    "print('| train set data to tensor..... |')\n",
    "## To tensors\n",
    "for i in range(len(train_set)):\n",
    "    X = x_train_data[i]\n",
    "    Y = y_train_data[i]\n",
    "    X_ss = ss.fit_transform(X)\n",
    "    \n",
    "    X_tensors = torch.Tensor(abs(X_ss))\n",
    "    Y_tensors = torch.tensor(Y)\n",
    "\n",
    "    X_train_tensors.append(X_tensors.to(device))\n",
    "    Y_train_tensors.append(Y_tensors.to(device))\n",
    "print('| done |')\n",
    "\n",
    "print('| valid data to tensor..... |')\n",
    "for i in range(len(valid_set)):\n",
    "    X = x_valid_data[i]\n",
    "    Y = y_valid_data[i]\n",
    "    X_ss = ss.fit_transform(X)\n",
    "    \n",
    "    X_tensors = torch.Tensor(abs(X_ss))\n",
    "    Y_tensors = torch.tensor(Y)\n",
    "\n",
    "    X_valid_tensors.append(X_tensors.to(device))\n",
    "    Y_valid_tensors.append(Y_tensors.to(device))\n",
    "print('| done |')\n",
    "\n",
    "print('| test data to tensor..... |')\n",
    "for i in range(len(test_set)):\n",
    "    X = x_test_data[i]\n",
    "    Y = y_test_data[i]\n",
    "    X_ss = ss.fit_transform(X)\n",
    "    \n",
    "    X_tensors = torch.Tensor(abs(X_ss))\n",
    "    Y_tensors = torch.tensor(Y)\n",
    "\n",
    "    X_test_tensors.append(X_tensors.to(device))\n",
    "    Y_test_tensors.append(Y_tensors.to(device))\n",
    "print('| done |')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "\n",
    "n_features = len(Index) #number of features\n",
    "n_hidden = 32 #number of features in hidden state\n",
    "n_layers = 2 #number of stacked lstm layers\n",
    "n_classes = 6 #number of output classes \n",
    "dropout = 0.2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f56ef7c2d0bd40458fdc227aa2de5402"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | train loss: 1.79638, best: 1.79638 | accuracy: 0.16500, best: 0.16500\n",
      "         | valid loss: 1.79638, best: 1.78532 | accuracy: 0.16500, best: 0.20000\n",
      "Epoch: 1 | train loss: 1.79562, best: 1.79562 | accuracy: 0.16500, best: 0.16500\n",
      "         | valid loss: 1.79562, best: 1.78532 | accuracy: 0.16500, best: 0.22000\n",
      "Epoch: 2 | train loss: 1.79480, best: 1.79480 | accuracy: 0.16500, best: 0.16500\n",
      "         | valid loss: 1.79480, best: 1.78532 | accuracy: 0.16500, best: 0.22000\n",
      "Epoch: 3 | train loss: 1.79453, best: 1.79453 | accuracy: 0.15500, best: 0.16500\n",
      "         | valid loss: 1.79453, best: 1.78532 | accuracy: 0.15500, best: 0.22000\n",
      "Epoch: 4 | train loss: 1.79373, best: 1.79373 | accuracy: 0.17500, best: 0.17500\n",
      "         | valid loss: 1.79373, best: 1.78532 | accuracy: 0.17500, best: 0.22000\n",
      "Epoch: 5 | train loss: 1.79319, best: 1.79319 | accuracy: 0.19500, best: 0.19500\n",
      "         | valid loss: 1.79319, best: 1.78532 | accuracy: 0.19500, best: 0.22000\n",
      "Epoch: 6 | train loss: 1.79315, best: 1.79315 | accuracy: 0.17500, best: 0.19500\n",
      "         | valid loss: 1.79315, best: 1.78532 | accuracy: 0.17500, best: 0.22000\n",
      "Epoch: 7 | train loss: 1.79233, best: 1.79233 | accuracy: 0.18000, best: 0.19500\n",
      "         | valid loss: 1.79233, best: 1.78532 | accuracy: 0.18000, best: 0.22000\n",
      "Epoch: 8 | train loss: 1.79260, best: 1.79233 | accuracy: 0.18000, best: 0.19500\n",
      "         | valid loss: 1.79260, best: 1.78532 | accuracy: 0.18000, best: 0.22000\n",
      "Epoch: 9 | train loss: 1.79177, best: 1.79177 | accuracy: 0.18000, best: 0.19500\n",
      "         | valid loss: 1.79177, best: 1.78532 | accuracy: 0.18000, best: 0.22000\n",
      "Epoch: 10| train loss: 1.79183, best: 1.79177 | accuracy: 0.18000, best: 0.19500\n",
      "         | valid loss: 1.79183, best: 1.78532 | accuracy: 0.18000, best: 0.22000\n",
      "Epoch: 11| train loss: 1.79091, best: 1.79091 | accuracy: 0.18000, best: 0.19500\n",
      "         | valid loss: 1.79091, best: 1.78532 | accuracy: 0.18000, best: 0.22000\n",
      "Epoch: 12| train loss: 1.79123, best: 1.79091 | accuracy: 0.18000, best: 0.19500\n",
      "         | valid loss: 1.79123, best: 1.78532 | accuracy: 0.18000, best: 0.22000\n",
      "Epoch: 13| train loss: 1.79061, best: 1.79061 | accuracy: 0.18000, best: 0.19500\n",
      "         | valid loss: 1.79061, best: 1.78532 | accuracy: 0.18000, best: 0.22000\n",
      "Epoch: 14| train loss: 1.79001, best: 1.79001 | accuracy: 0.18000, best: 0.19500\n",
      "         | valid loss: 1.79001, best: 1.78532 | accuracy: 0.18000, best: 0.22000\n",
      "Epoch: 15| train loss: 1.78990, best: 1.78990 | accuracy: 0.18500, best: 0.19500\n",
      "         | valid loss: 1.78990, best: 1.78532 | accuracy: 0.18500, best: 0.22000\n",
      "Epoch: 16| train loss: 1.78996, best: 1.78990 | accuracy: 0.18000, best: 0.19500\n",
      "         | valid loss: 1.78996, best: 1.78532 | accuracy: 0.18000, best: 0.22000\n",
      "Epoch: 17| train loss: 1.78921, best: 1.78921 | accuracy: 0.18500, best: 0.19500\n",
      "         | valid loss: 1.78921, best: 1.78532 | accuracy: 0.18500, best: 0.22000\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(n_features, n_classes, n_hidden, n_layers, dropout, device) #our lstm class\n",
    "lstm.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate) \n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=6)\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "\n",
    "trainer_path = 'G:/Workspace/TENG-Signal-Classification/trainer'\n",
    "\n",
    "valid_loss = []\n",
    "\n",
    "valid_acc = []\n",
    "\n",
    "to_one_hot = [[1,0,0,0,0,0],[0,1,0,0,0,0],[0,0,1,0,0,0],[0,0,0,1,0,0],[0,0,0,0,1,0],[0,0,0,0,0,1]]\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    l = []\n",
    "    pre = []\n",
    "    # mix = []\n",
    "    log = {}\n",
    "    liveloss = PlotLosses()\n",
    "    \n",
    "    for index, case in enumerate(train_set):\n",
    "        # rand_index = random.randint(0,len(train_set) - 1)\n",
    "        x1 = X_train_tensors[index]\n",
    "        # x2 = X_train_tensors[rand_index]\n",
    "        label1 = Y_train_tensors[index]\n",
    "        # label2 = Y_train_tensors[rand_index]\n",
    "        one_hot1 = to_one_hot[label1]\n",
    "        # one_hot2 = to_one_hot[label2]\n",
    "\n",
    "        # lam = np.random.beta(1, 1)\n",
    "        # x_mixed_up = lam * x1 + (1 - lam) * x2\n",
    "        # y_mixed_up = lam * torch.FloatTensor(one_hot1) + (1 - lam) * torch.FloatTensor(one_hot2)\n",
    "\n",
    "        outputs = lstm.forward(x1) #forward pass\n",
    "        optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "\n",
    "        # obtain the loss function\n",
    "        step_loss = criterion(torch.unsqueeze(outputs,0), torch.unsqueeze(torch.FloatTensor(one_hot1),0).to(device))\n",
    "        l.append(step_loss.item())\n",
    "\n",
    "        outputs = outputs.softmax(dim = 0)\n",
    "        prediction = torch.argmax(outputs)\n",
    "        pre.append(prediction)\n",
    "        step_loss.backward() #calculates the loss of the loss function\n",
    "        optimizer.step() #improve from loss, i.e backprop\n",
    "\n",
    "    acc = accuracy(torch.tensor(pre), torch.tensor(Y_train_tensors))\n",
    "    train_acc.append(acc.item())\n",
    "    loss = np.mean(l) \n",
    "    train_loss.append(loss)\n",
    "    best_train_loss = min(train_loss)\n",
    "    \n",
    "    if epoch < 10:\n",
    "        print(\"Epoch: %d | train loss: %1.5f, best: %1.5f | accuracy: %1.5f, best: %1.5f\" % (epoch, loss, best_train_loss, acc, max(train_acc))) \n",
    "    else:\n",
    "        print(\"Epoch: %d| train loss: %1.5f, best: %1.5f | accuracy: %1.5f, best: %1.5f\" % (epoch, loss, best_train_loss, acc, max(train_acc))) \n",
    "\n",
    "    # if epoch % 5 == 0 or epoch == num_epochs - 1 :\n",
    "    _l = []\n",
    "    _pre = []\n",
    "    _acc = []\n",
    "    for index, case in enumerate(valid_set):\n",
    "\n",
    "        label = Y_valid_tensors[index]\n",
    "        one_hot = to_one_hot[label]\n",
    "        \n",
    "        outputs = lstm(X_valid_tensors[index]) #forward pass\n",
    "        step_loss = criterion(torch.unsqueeze(outputs,0), torch.unsqueeze(torch.FloatTensor(one_hot),0).to(device))\n",
    "        _l.append(step_loss.item())\n",
    "         \n",
    "        outputs = outputs.softmax(dim = 0)\n",
    "        prediction = torch.argmax(outputs)\n",
    "        \n",
    "        _pre.append(prediction)\n",
    "    \n",
    "    _acc = accuracy(torch.tensor(_pre), torch.tensor(Y_valid_tensors)) \n",
    "    valid_acc.append(_acc.item())\n",
    "    _loss = np.mean(_l)\n",
    "    if  acc >= 0.8:\n",
    "        torch.save(lstm.state_dict(), trainer_path + 'epoch_' + str(epoch) +\n",
    "                   '_loss_' + str(_loss) + '_acc_' + str(_acc.item() * 100) + '%' + '.pt')\n",
    "    \n",
    "    valid_loss.append(_loss)\n",
    "    best_valid_loss = min(valid_loss)\n",
    "    \n",
    "    if epoch == num_epochs - 1:\n",
    "        print(\"         | valid loss: %1.5f, best: %1.5f | accuracy: %1.5f, best: %1.5f\" % (loss, best_valid_loss, acc, max(valid_acc))) \n",
    "    else:\n",
    "         print(\"         | valid loss: %1.5f, best: %1.5f | accuracy: %1.5f, best: %1.5f\" % (loss, best_valid_loss, acc, max(valid_acc)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-18T10:07:09.728105Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = 'D:/Workspace/Teng-Signal-Classification/output/'\n",
    "\n",
    "csv_file = path + 'train_loss' + '.csv'\n",
    "img_file = path + 'train_loss' + '.jpg'\n",
    "\n",
    "epoch_scale = list(range(0, len(train_loss)))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_scale, train_loss, epoch_scale, valid_loss)\n",
    "label = ['train loss', 'valid loss']\n",
    "plt.legend(label, loc='upper right')\n",
    "plt.savefig(path + 'loss.jpg')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_scale, train_acc, epoch_scale, valid_acc)\n",
    "label = ['train acc', 'valid acc']\n",
    "plt.legend(label, loc='upper right')\n",
    "plt.savefig(path + 'acc.jpg')\n",
    "plt.show()\n",
    "\n",
    "Data = {\n",
    "    'train_loss': [],\n",
    "    'valid_loss': [],\n",
    "    'train_acc': [],\n",
    "    'valid_acc': [],\n",
    "\n",
    "}\n",
    "\n",
    "Data['train_loss'] = train_loss\n",
    "Data['valid_loss'] = valid_loss\n",
    "Data['train_acc'] = train_acc\n",
    "Data['valid_acc'] = valid_acc\n",
    "\n",
    "savedata = pd.DataFrame(data=Data)\n",
    "savedata.to_csv(path + 'loss&acc.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
